{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8964b3",
   "metadata": {},
   "source": [
    "#### Login to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ac66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"YOUR_HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd022be5",
   "metadata": {},
   "source": [
    "#### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b33572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"google/gemma-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f35f94",
   "metadata": {},
   "source": [
    "#### Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from peft import PeftModel\n",
    "# import torch\n",
    "\n",
    "# base = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"google/gemma-2b-it\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gemma-lora-webq-finetuned\")\n",
    "\n",
    "# model = PeftModel.from_pretrained(base, \"gemma-lora-webq-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c413d0",
   "metadata": {},
   "source": [
    "#### Load Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec67e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"knowledge_graph.csv\")\n",
    "\n",
    "filtered_df = df[[\"subject_label\", \"predicate_label\", \"object_label\"]].copy()\n",
    "\n",
    "filtered_df['subject_label'] = filtered_df['subject_label'].astype(str).str.strip()\n",
    "filtered_df['predicate_label'] = filtered_df['predicate_label'].astype(str).str.strip()\n",
    "filtered_df['object_label'] = filtered_df['object_label'].astype(str).str.strip()\n",
    "\n",
    "filtered_df['subject_label_lower'] = filtered_df['subject_label'].str.lower()\n",
    "filtered_df['predicate_label_lower'] = filtered_df['predicate_label'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f1075",
   "metadata": {},
   "source": [
    "#### Grab unique Subjects & Predicates from Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44785546",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list = filtered_df['subject_label_lower'].unique()\n",
    "predicate_list = filtered_df['predicate_label_lower'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef87d7c",
   "metadata": {},
   "source": [
    "#### Extract Facts from Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def extract_best_match(text, candidates, cutoff=0.6):\n",
    "    text = text.lower()\n",
    "\n",
    "    best_matches = difflib.get_close_matches(text, candidates, n=1, cutoff=cutoff)\n",
    "    if best_matches:\n",
    "        return best_matches[0]\n",
    "    \n",
    "    matches = [c for c in candidates if c in text]\n",
    "    return max(matches, key=len) if matches else None\n",
    "\n",
    "def get_facts_for_question(question, df, subject_list, predicate_list):\n",
    "\n",
    "    matched_subject = extract_best_match(question, subject_list)\n",
    "    if matched_subject is None:\n",
    "        return []\n",
    "\n",
    "    subject_df = df[df[\"subject_label_lower\"] == matched_subject]\n",
    "    if subject_df.empty:\n",
    "        return []\n",
    "\n",
    "    matched_predicate = extract_best_match(question, predicate_list)\n",
    "\n",
    "    if matched_predicate:\n",
    "\n",
    "        filtered_df = subject_df[\n",
    "            subject_df[\"predicate_label\"].str.lower() == matched_predicate\n",
    "        ]\n",
    "        if not filtered_df.empty:\n",
    "            facts = [\n",
    "                f\"{row['subject_label']} — {row['predicate_label']} — {row['object_label']}\"\n",
    "                for _, row in filtered_df.iterrows()\n",
    "            ]\n",
    "            return facts\n",
    "\n",
    "    facts = [\n",
    "        f\"{row['subject_label']} — {row['predicate_label']} — {row['object_label']}\"\n",
    "        for _, row in subject_df.iterrows()\n",
    "    ]\n",
    "    return facts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe082e",
   "metadata": {},
   "source": [
    "#### Generate Answer using facts from Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb45867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, facts):\n",
    "    formatted_facts = \"\\n\".join(f\"- {fact}\" for fact in facts)\n",
    "    prompt = f\"\"\"<bos>\n",
    "[INST]\n",
    "Using the facts below, answer the question with a short, direct answer.\n",
    "\n",
    "Facts:\n",
    "{formatted_facts}\n",
    "\n",
    "Question: {question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"[/INST]\" in full_output:\n",
    "        answer_only = full_output.split(\"[/INST]\")[-1].strip()\n",
    "    else:\n",
    "        answer_only = full_output.strip()\n",
    "\n",
    "    return answer_only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ea705",
   "metadata": {},
   "source": [
    "#### Test Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"what state does selena gomez?\"\n",
    "\n",
    "# facts = get_facts_for_question(question, filtered_df, subject_list, predicate_list)\n",
    "\n",
    "# answer = generate_answer(question, facts)\n",
    "\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d12d86",
   "metadata": {},
   "source": [
    "#### Questions from WebQuestions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/web_questions\", split=\"test\")\n",
    "\n",
    "questions_df = pd.DataFrame(dataset)[['question', 'answers']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e0537",
   "metadata": {},
   "source": [
    "#### Process One Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c276ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question):\n",
    "    facts = get_facts_for_question(question, filtered_df, subject_list, predicate_list)\n",
    "    if not facts:\n",
    "        facts = [\"No facts available.\"]\n",
    "    return generate_answer(question, facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b60334",
   "metadata": {},
   "source": [
    "#### Answer all questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9109930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "questions_df['predicted_answer'] = questions_df['question'].progress_apply(process_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633effa2",
   "metadata": {},
   "source": [
    "#### Prepare Data for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_bleu = questions_df['answers'].tolist()\n",
    "references_rouge = questions_df['answers'].apply(lambda x: ' / '.join(x)).tolist()\n",
    "\n",
    "predictions = questions_df['predicted_answer'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13027878",
   "metadata": {},
   "source": [
    "#### Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13444c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "bleu_res = bleu.compute(predictions=predictions, references=references_bleu)\n",
    "rouge_res = rouge.compute(predictions=predictions, references=references_rouge)\n",
    "\n",
    "results_df = pd.DataFrame([{\n",
    "    \"bleu\":      bleu_res[\"bleu\"],\n",
    "    \"rouge1\":    rouge_res[\"rouge1\"],\n",
    "    \"rouge2\":    rouge_res[\"rouge2\"],\n",
    "    \"rougeL\":    rouge_res[\"rougeL\"],\n",
    "    \"rougeLsum\": rouge_res[\"rougeLsum\"],\n",
    "}])\n",
    "\n",
    "results_df.to_csv(\"evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b5912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
