{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa91bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install peft transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Gemma-2B (Base for Knowledge Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae898bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & setup\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from preprocess import preprocess\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from manual_eval import manual_eval   \n",
    "import torch\n",
    "\n",
    "import importlib\n",
    "import manual_eval\n",
    "importlib.reload(manual_eval)\n",
    "from manual_eval import manual_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df881a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & split raw data\n",
    "dataset = load_dataset(\"stanfordnlp/web_questions\")\n",
    "\n",
    "# drop any examples with no answer\n",
    "dataset = dataset.filter(lambda ex: len(ex[\"answers\"]) > 0)\n",
    "\n",
    "split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "raw_train_dataset = split[\"train\"]\n",
    "raw_test_dataset  = split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67fde81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: stanfordnlp/web_questions from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f313d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize (keeping raw_* intact)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "    # update model's pad_token_id after loading model…\n",
    "\n",
    "# map with preprocess(), which returns input_ids, attention_mask, labels\n",
    "tokenized_train = raw_train_dataset.map(\n",
    "    lambda ex: preprocess(ex, tokenizer),\n",
    "    batched=True,\n",
    "    batch_size=256,\n",
    "    remove_columns=raw_train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_test = raw_test_dataset.map(\n",
    "    lambda ex: preprocess(ex, tokenizer),\n",
    "    remove_columns=raw_test_dataset.column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfccd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, LoRA, Trainer setup\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# ensure pad token is set\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config._attn_implementation = \"eager\"\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # confirm only LoRA params are trainable\n",
    "model.gradient_checkpointing_disable()\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-lora-webq\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=1,   # safe eval\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,             \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Baseline manual eval on 100 examples\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "baseline_metrics = manual_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    raw_test_dataset,  # use the RAW split so ex[\"question\"] exists\n",
    "    slice_size=100     # fast sanity‐check on 100 samples\n",
    ")\n",
    "\n",
    "print(\"Baseline (100 ex):\", baseline_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ff19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [To run later!!] Cell: Full evaluation on entire test set\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "full_metrics = manual_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    raw_test_dataset,\n",
    "    slice_size=len(raw_test_dataset)\n",
    ")\n",
    "print(\"Full evaluation:\", full_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,              \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1cc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-training manual eval on same 100 examples\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "finetuned_metrics = manual_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    raw_test_dataset,\n",
    "    slice_size=100\n",
    ")\n",
    "print(\"After fine-tuning (100 ex): \", finetuned_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Full evaluation on entire test set\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "full_metrics = manual_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    raw_test_dataset,\n",
    "    slice_size=len(raw_test_dataset)\n",
    ")\n",
    "print(\"Full evaluation: \", full_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model & tokenizer\n",
    "trainer.save_model(\"gemma-lora-webq-finetuned\")\n",
    "tokenizer.save_pretrained(\"gemma-lora-webq-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48694aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Graph Experiment [Exploring - will need to make it more robust]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a70313",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install SPARQLWrapper pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759de100",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "%pip install spacy-wikidata\n",
    "%python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d85376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_wikidata import WikidataEntityLinker\n",
    "\n",
    "# Load the spaCy model with Wikidata component\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "linker = WikidataEntityLinker(\n",
    "    name=\"wikidata\",\n",
    "    url=\"https://query.wikidata.org/sparql\",\n",
    "    entity_linker=\"wikidata\",\n",
    "    resolve_entities=True,\n",
    ")\n",
    "\n",
    "nlp.add_pipe(\"wikidata\", config={\"use_cache\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762205d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"What is electricity?\")\n",
    "\n",
    "qid = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in doc.ents:\n",
    "    if e.kb_id_:\n",
    "        print(f\"Entity: {e.text}, Wikidata ID: {e.kb_id_}, Label: {e.label_}\")\n",
    "        qid = e.kb_id_.split(\"/\")[-1]\n",
    "    else:\n",
    "        print(f\"Entity: {e.text} (no Wikidata ID found)\")\n",
    " # Get the QID from the first entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33608ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPARQL query to get related entities\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def query_wikidata_by_qid(qid):\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    sparql.setQuery(f\"\"\"\n",
    "    SELECT ?item ?itemLabel WHERE {{\n",
    "        wd:{qid} wdt:P31 ?item .\n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    \"\"\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    query = sparql.queryString\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03207af",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_wikidata_by_qid(qid)\n",
    "print(\"SPARQL Query:\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to work on SPARQL + knowledge graph integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can reuse Gemma and its tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
